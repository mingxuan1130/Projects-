{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA):\n",
        "\n"
      ],
      "metadata": {
        "id": "784XvKNt9bnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a method of dimension reduction.\n",
        "\n",
        "Dimension reduction in data analysis is crucial when handling a large number of variables. Often, these many variables do not provide additional insight, making the process burdensome.\n",
        "\n",
        "Consider a dataset about different cars, including their weight, engine size, horsepower, and top speed. These features might be correlated, such as heavier cars often having bigger engines. From a mathematical perspective, these features are not orthogonal to each other; some features might be partially represented by others.\n",
        "\n",
        "PCA assists by creating new variables that summarize these features. It aims to combining ingredients(features) in a recipe; the final dish (new variables or principle compoments) retains the essence of the individual ingredients (original features), but in a more condensed form."
      ],
      "metadata": {
        "id": "-E6Jdk92AoiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a dataset $A$ of dimensions $(m, n)$, suppose A is tall i.e., $m > n$, the columns represent features, and the rows represent samples. The covariance matrix $C$ of $A$ quantifies the extent of correlation among these features. It can be calculated using the formula $C = \\frac{1}{m-1}\\tilde{A}^T\\tilde{A}$, where $\\tilde{A}$ denotes the mean-centered data of $A$, and $m$ is the number of samples. If $C_{1,2} = C_{2,1}$ are positive, it indicates a positive correlation between the first and the second feature."
      ],
      "metadata": {
        "id": "Ud6di40GKj9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsequently, a Singular Value Decomposition (SVD) of $C$ is performed. Let us assume $\\tilde{A} = U\\Sigma V^T$ and $\\tilde{A}^{T} = V\\Sigma U^T$. Then, $\\tilde{A}^T\\tilde{A} = V\\Sigma U^TU\\Sigma V^T = V\\text{diag}(\\sigma^2_{1},\\dots, \\sigma^2_{n}) V^T$. Hence, we can infer that $V = \\begin{bmatrix}| & | & & | \\\\ \\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n \\\\ | & | & & | \\end{bmatrix}$ forms an eigenspace of $\\tilde{A}$, where $\\mathbf{v}_1, \\dots, \\mathbf{v}_n$ are known as the principal directions of $\\tilde{A}$.\\\n",
        "It is observed that the SVD of $C$ is essentially $\\frac{1}{m-1}V\\text{diag}(\\sigma^2_{1},\\dots, \\sigma^2_{n})V^{T}$, and the directions of $\\mathbf{v}_1, \\dots, \\mathbf{v}_n$ are the projections we seek."
      ],
      "metadata": {
        "id": "QitjUcoB-tgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summary:\n",
        "\n",
        "Suppose we are given a large data set $A$ of dimension $m \\times n$ $(m>n)$, and we want to reduce the data set to a smaller one $\\hat{A}$ of dimension $m \\times k$ without loss of important information. We can achieve this by carrying out PCA algorithm with the following steps:\n",
        "\n",
        "1. Shift the data set $A$ so that it has zero mean: $\\tilde{A} = A - np.mean(A, axis=0)$.\n",
        "2. Compute SVD for the original data set: $\\tilde{A} = U\\Sigma V^T$.\n",
        " Note that the variance of the data set are determined by the singular values of $A$, i.e., $\\sigma_1, \\dots, \\sigma_n$. Note that the columns of $V$ represent the principal directions of the dataset and their corresponding variance are $\\sigma^{2}_{1}, \\dots, \\sigma^{2}_{n}$\n",
        "3. Our dataset after the projection with $k$ principle directions is\n",
        "$$Y := \\tilde{A}\\begin{bmatrix}| & | & & | \\\\ \\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_k \\\\ | & | & & | \\end{bmatrix} = \\begin{bmatrix}| & | & & | \\\\ \\mathbf{u}_1 & \\mathbf{u}_2 & \\cdots & \\mathbf{u}_k \\\\ | & | & & | \\end{bmatrix} \\begin{bmatrix}\n",
        "\\sigma_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_k\n",
        "\\end{bmatrix}\n",
        "$$.\n",
        "If we want to reduce the dimension of the data set, and only use the most important $k$ principal directions, now we've got the desired $Y$, we're done. However, if we want to convert the data back to the original dimension...\n",
        "4. We need one more step to reconstruct $\\hat{A}$ by:\n",
        " $$\\hat{A} = Y \\begin{bmatrix} --- & \\mathbf{v}_1^T &--- \\\\ --- & \\mathbf{v}_2^T & --- \\\\ & \\vdots & \\\\ --- & \\mathbf{v}_k^T & --- \\end{bmatrix}$$.\\\n",
        " Equivalently,\\\n",
        " $$\\hat{A} = \\tilde{A}\\begin{bmatrix}| & | & & | \\\\ \\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_k \\\\ | & | & & | \\end{bmatrix} \\begin{bmatrix} --- & \\mathbf{v}_1^T &--- \\\\ --- & \\mathbf{v}_2^T & --- \\\\ & \\vdots & \\\\ --- & \\mathbf{v}_k^T & --- \\end{bmatrix}$$."
      ],
      "metadata": {
        "id": "9XQGifD3AD_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretically, Principal Component Analysis (PCA) aims to identify a linear subspace in which the variance of the projected data is maximized, thereby preserving the most significant information. This objective is accomplished through the application of Singular Value Decomposition (SVD), resulting in getting the principal components. These components are characterized by maximized variances, which are $\\sigma^{2}_{i}$.\n",
        "\n",
        "Alternatively, PCA can be viewed from the perspective of minimizing the \"reconstruction error\", which is formally defined by the equation $||X-\\hat{X}||^{2}$.\n",
        "\n",
        "Interestingly, the two ways we look at PCA, either as a method to get the most information out of our data by maximizing the variance in the projected space, or as a way to minimize the error when we try to reconstruct the original data, are actually the same thing. This comes from the fact that, when we keep the size of our data samples the same, making sure we capture as much variation as possible in our projection automatically means we're also minimizing reconstruction error."
      ],
      "metadata": {
        "id": "xTlct87_kUcG"
      }
    }
  ]
}